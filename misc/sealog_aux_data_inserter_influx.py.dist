#!/usr/bin/env python3
'''
FILE:           sealog_aux_data_inserter_influx.py

DESCRIPTION:    This service listens for new events submitted to Sealog, create
                aux_data records containing the specified real-time data and
                associates the aux data records with the newly created event.

                This script leverages the OpenRVDAS/InfluxDB integration so that
                if can add ancillary data from any point in time so long as the
                data is availble from the InfluxDB.  In the event the data is not
                available the script will NOT add the corresponding aux_data
                records.

                This script can also add influx data to a list of event ids
                (comma-separated) using the -e flag, or all the events for a given
                lowering using the -l flag, or all the events for a given
                cruise using the -c flag

BUGS:
NOTES:
AUTHOR:     Webb Pinner
COMPANY:    OceanDataTools.org
VERSION:    1.1
CREATED:    2021-04-21
REVISION:   2023-02-10

LICENSE INFO:   This code is licensed under MIT license (see LICENSE.txt for details)
                Copyright (C) OceanDataTools.org 2024
'''

import re
import sys
import json
import time
import logging
import asyncio
import websockets
import yaml
import requests
from influxdb_client import InfluxDBClient

from os.path import dirname, realpath
sys.path.append(dirname(dirname(realpath(__file__))))

from misc.python_sealog.events import get_event, get_events_by_cruise, get_events_by_lowering
from misc.python_sealog.lowerings import get_lowering_uid_by_id
from misc.python_sealog.cruises import get_cruise_uid_by_id

from misc.python_sealog.settings import API_SERVER_URL, WS_SERVER_URL, LOWERINGS_API_PATH, EVENTS_API_PATH, EVENT_AUX_DATA_API_PATH, HEADERS
from misc.influx_sealog.settings import INFLUXDB_URL, INFLUXDB_AUTH_TOKEN, INFLUXDB_ORG, INFLUXDB_VERIFY_SSL
from misc.influx_sealog.aux_data_record_builder import SealogInfluxAuxDataRecordBuilder

#-----------------------------------------------------------------------------#

INLINE_CONFIG = '''
- data_source: realtimeVesselPosition
  # alarm:
  #   active: true
  #   interval: 300
  #   event:
  #     event_value: PROBLEM
  #     event_free_text: >-
  #       No vessel position data available from InfluxDB. Verify OpenRVDAS is
  #       sending Seapath data to InfluxDB
  #     event_options:
  #       - event_option_name: System
  #         event_option_value: Data
  query_measurements:
    - seapath1
  aux_record_lookup:
    S1HeadingTrue:
      name: heading
      uom: deg
      round: 3
    S1Latitude:
      name: latitude
      uom: ddeg
      round: 6
      modify:
        - test:
            - field: S1NorS
              eq: S
          operation:
            - multiply: -1
    S1Longitude:
      name: longitude
      uom: ddeg
      round: 6
      modify:
        - test:
            - field: S1EorW
              eq: W
          operation:
            - multiply: -1
    S1NorS:
      no_output: true
    S1EorW:
      no_output: true
'''

# set of events to ignore
EXCLUDE_SET = ()

# needs to be unique for all currently active dataInserter scripts.
CLIENT_WSID = 'auxData-dataInserter-influx'

HELLO = {
    'type': 'hello',
    'id': CLIENT_WSID,
    'auth': {
        'headers': HEADERS
    },
    'version': '2',
    'subs': ['/ws/status/newEvents']
}

PING = {
    'type':'ping',
    'id':CLIENT_WSID
}


def parse_event_ids(event_id_file):
    event_ids = list()
    with open(event_id_file) as file:
        lines = list()

        for line in file:
            line = line.rstrip('\n')
            logging.debug(line)
            event_ids += line.split(',')

        event_ids = [event_id.strip() for event_id in event_ids]

        for event_id in event_ids:
            if re.match(r"^[a-f\d]{24}$", event_id) is None:
                logging.error("\"%s\" is an invalid event_id... quiting", event_id)
                raise ValueError("\"%s\" is an invalid event_id... quiting", event_id)

    return event_ids


def insert_aux_data(aux_data_builders, event):
    for builder in aux_data_builders:
        logging.debug("Building aux data record")
        record = builder.build_aux_data_record(event)
        if record:
            try:
                logging.debug("Submitting aux data record to Sealog Server")
                logging.debug(json.dumps(record))
                req = requests.post(API_SERVER_URL + EVENT_AUX_DATA_API_PATH, headers=HEADERS, data = json.dumps(record))
                logging.debug("Response: %s", req.text)

            except Exception as err:
                logging.warning("Error submitting aux data record")
                logging.debug(str(err))
        else:
            logging.debug("No aux data for data_source: %s", builder.data_source)


def insert_aux_data_from_list(aux_data_builders, event_ids):
    for event_id in event_ids:
        try:
            logging.debug("Retrieving event record from Sealog Server")
            event = get_event(event_id)
            logging.debug("Event: %s", event)

        except Exception as err:
            logging.warning("Error submitting aux data record")
            logging.debug(str(err))
            raise(err)

        insert_aux_data(aux_data_builders, event)


def insert_aux_data_for_cruise(aux_data_builders, cruise_id, dry_run=False):
    cruise_uid = get_cruise_uid_by_id(cruise_id)

    # exit if no cruise found
    if not cruise_uid:
        logging.error("cruise not found")
        return None

    # retrieve events for cruise
    cruise_events = get_events_by_cruise(cruise_uid)

    # exit if no cruise found
    if not cruise_events:
        logging.error("no events found for cruise")
        return None

    for event in cruise_events:
        insert_aux_data(aux_data_builders, event)


def insert_aux_data_for_lowering(aux_data_builders, lowering_id, dry_run=False):
    lowering_uid = get_lowering_uid_by_id(lowering_id)

    # exit if no lowering found
    if not lowering_uid:
        logging.error("lowering not found")
        return None

    # retrieve events for lowering
    lowering_events = get_events_by_lowering(lowering_uid)

    # exit if no lowering found
    if not lowering_events:
        logging.error("no events found for lowering")
        return None

    for event in lowering_events:
        insert_aux_data(aux_data_builders, event)


async def insert_aux_data_from_ws(aux_data_builders):
    '''
    Use the aux_data_builder and the influx_sealog wrapper to submit aux_data
    records built from influxDB data to the sealog-server API
    '''

    try:
        async with websockets.connect(WS_SERVER_URL) as websocket:

            await websocket.send(json.dumps(HELLO))

            while True:

                event = await websocket.recv()
                event_obj = json.loads(event)

                if event_obj['type'] and event_obj['type'] == 'ping':
                    await websocket.send(json.dumps(PING))
                elif event_obj['type'] and event_obj['type'] == 'pub':

                    if event_obj['message']['event_value'] in EXCLUDE_SET:
                        logging.debug("Skipping because event value is in the exclude set")
                        continue

                    logging.debug("Event: %s", event_obj['message'])

                    insert_aux_data(aux_data_builders, event_obj['message'])



    except Exception as err:
        logging.error(str(err))
        raise err

# -------------------------------------------------------------------------------------
# The main loop of the utility
# -------------------------------------------------------------------------------------
if __name__ == '__main__':

    import argparse
    import os

    parser = argparse.ArgumentParser(description='Aux Data Inserter Service - InfluxDB')
    parser.add_argument('-v', '--verbosity', dest='verbosity',
                        default=0, action='count',
                        help='Increase output verbosity')
    parser.add_argument('-f', '--config_file', help='use the specifed configuration file')
    parser.add_argument('-n', '--dry_run', action='store_true', help='compile the aux_data records but do not submit to server API')
    parser.add_argument('-e', '--events', help='list of event_ids to apply the influx data')
    parser.add_argument('-c', '--cruise_id', help='cruise_id to fix aux_data for')
    parser.add_argument('-l', '--lowering_id', help='lowering_id to fix aux_data for')

    parsed_args = parser.parse_args()

    ############################
    # Set up logging before we do any other argument parsing (so that we
    # can log problems with argument parsing).

    LOGGING_FORMAT = '%(asctime)-15s %(levelname)s - %(message)s'
    logging.basicConfig(format=LOGGING_FORMAT)

    LOG_LEVELS = {0: logging.WARNING, 1: logging.INFO, 2: logging.DEBUG}
    parsed_args.verbosity = min(parsed_args.verbosity, max(LOG_LEVELS))
    logging.getLogger().setLevel(LOG_LEVELS[parsed_args.verbosity])

    aux_data_configs = None # pylint: disable=invalid-name

    if parsed_args.config_file:

        config_file, data_source = parsed_args.config_file.split(":") if ":" in parsed_args.config_file else [ parsed_args.config_file, None ]
        
        try:
            with open(config_file) as file:
                aux_data_configs = yaml.safe_load(file)
    
            if data_source:
                aux_data_configs = [config for config in aux_data_configs if config['data_source'] == data_source]

        except yaml.parser.ParserError:
            logging.error("Invalid YAML syntax")
            sys.exit(1)
    else:
        try:
            aux_data_configs = yaml.safe_load(INLINE_CONFIG)
        except yaml.parser.ParserError:
            logging.error("Invalid YAML syntax")
            sys.exit(1)

    logging.debug(json.dumps(aux_data_configs, indent=2))

    # create an influxDB Client
    use_ssl = INFLUXDB_URL.find('https:') == 0
    client = InfluxDBClient(url=INFLUXDB_URL,
                            token=INFLUXDB_AUTH_TOKEN,
                            org=INFLUXDB_ORG,
                            ssl=use_ssl,
                            verify_ssl=INFLUXDB_VERIFY_SSL)

    # Create the Aux Data Record Builders
    aux_data_builder_list = list(map(lambda config: SealogInfluxAuxDataRecordBuilder(client, config), aux_data_configs))

    if parsed_args.events:
        logging.debug("Processing list of event ids")

        event_ids = parse_event_ids(parsed_args.events)
        logging.info("Event IDs:\n%s",json.dumps(event_ids, indent=2))

        insert_aux_data_from_list(aux_data_builder_list, event_ids)

        sys.exit(0)

    if parsed_args.cruise_id:
        logging.debug("Processing events for an entire cruise")

        insert_aux_data_for_cruise(aux_data_builder_list, parsed_args.cruise_id, parsed_args.dry_run)

        sys.exit(0)

    if parsed_args.lowering_id:
        logging.debug("Processing events for an entire lowering")

        insert_aux_data_for_lowering(aux_data_builder_list, parsed_args.lowering_id, parsed_args.dry_run)

        sys.exit(0)

    # Run the main loop
    while True:

        # Wait 5 seconds for the server to complete startup
        time.sleep(5)

        try:
            logging.debug("Connecting to event websocket feed...")
            asyncio.get_event_loop().run_until_complete(insert_aux_data_from_ws(aux_data_builder_list))
        except KeyboardInterrupt:
            logging.error('Keyboard Interrupted')
            try:
                sys.exit(0)
            except SystemExit:
                os._exit(0) # pylint: disable=protected-access
        except Exception as err:
            logging.debug(str(err))
            logging.error("Lost connection to server, trying again in 5 seconds")